# RAGç³»ç»Ÿ-æ¢ç´¢çš„ä¾‹å­-BTBU

<p align="center">
  <a href="./README.md">English</a> |
  <a href="./README_zh.md">ç®€ä½“ä¸­æ–‡</a>
</p>

è¿™ä¸ªä¾‹å­æ˜¯å¯¹åæœŸå·¥ä½œçš„åˆæ­¥æ¢ç´¢ã€‚å®ƒä½¿ç”¨Ollamaå’ŒLangchainç­‰æ¡†æ¶å·¥å…·åˆ¶ä½œäº†ä¸€ä¸ªç®€å•çš„ç¨‹åºï¼Œå¯ä»¥æ’å…¥çŸ¥è¯†åº“æ¥æ‰§è¡ŒRAGé—®ç­”ã€‚
### What is RAG?
LLM å¯ä»¥æ¨ç†å¹¿æ³›çš„ä¸»é¢˜ï¼Œä½†ä»–ä»¬çš„çŸ¥è¯†ä»…é™äºä»–ä»¬æ¥å—è®­ç»ƒçš„ç‰¹å®šæ—¶é—´ç‚¹çš„å…¬å…±æ•°æ®ã€‚å¦‚æœæ‚¨æƒ³æ„å»ºèƒ½å¤Ÿæ¨ç†ç§æœ‰æ•°æ®æˆ–æ¨¡å‹æˆªæ­¢æ—¥æœŸåå¼•å…¥çš„æ•°æ®çš„ AI åº”ç”¨ç¨‹åºï¼Œåˆ™éœ€è¦ä½¿ç”¨æ¨¡å‹æ‰€éœ€çš„ç‰¹å®šä¿¡æ¯æ¥å¢å¼ºæ¨¡å‹çš„çŸ¥è¯†ã€‚å°†é€‚å½“çš„ä¿¡æ¯å¼•å…¥æ¨¡å‹æç¤ºçš„è¿‡ç¨‹ç§°ä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)ã€‚
ğŸ”·æ£€ç´¢:ç”¨æˆ·é—®é¢˜ç”¨äºä»å¤–éƒ¨çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ã€‚ä¸ºæ­¤ï¼Œç”¨æˆ·æŸ¥è¯¢å°†è¢«åµŒå…¥åˆ°ä¸â€œå‘é‡æ•°æ®åº“ä¸­çš„ä¸Šä¸‹æ–‡â€ç›¸åŒçš„å‘é‡ç©ºé—´ä¸­ï¼Œç„¶ååœ¨æ­¤ç©ºé—´ä¸­è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œä»¥è¿”å›æ•°æ®åº“ä¸­æœ€ç›¸ä¼¼çš„å‰ k ä¸ªæ•°æ®å¯¹è±¡åˆ°æŸ¥è¯¢ã€‚

ğŸ”·å¢å¼º:ç”¨æˆ·æŸ¥è¯¢å’Œæ£€ç´¢çš„å†…å®¹è¢«å¡«å……åˆ°æç¤ºæ¨¡æ¿ä¸­ã€‚

ğŸ”·ç”Ÿæˆ:æœ€åï¼Œæ£€ç´¢å¢å¼ºçº¿ç´¢è¢«è¾“å…¥åˆ°å¤§é¢„è¨€æ¨¡å‹ä¸­ã€‚

## å‰æœŸå·¥ä½œ

<div align="left side">
Â <img alt="ollama" height="100px" src="https://github.com/ollama/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7">
</div>

### Ollama*
å¯åŠ¨å¹¶è¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚


#### Windows

[Download](https://ollama.com/download/OllamaSetup.exe)

#### macOS

[Download](https://ollama.com/download/Ollama-darwin.zip)

#### Model
è¿™ä¸ªä¾‹å­ç”¨åˆ°çš„æ˜¯qwen:4bæ¨¡å‹(åªæœ‰2.3G)
```
pip install qwen:4b
```
### ğŸ¦œï¸ğŸ”— LangChain*

#### Quick Install

With pip:
```
pip install langchain
```
With conda:
```
conda install langchain -c conda-forge
```
### PyMuPDF*
**PyMuPDF** éœ€è¦ **Python 3.8æˆ–è€…ä¹‹åçš„ç‰ˆæœ¬**,å®‰è£…å‘¢éœ€è¦ **pip** :
#### å®‰è£…
```
pip install PyMuPDF
```
### Chroma*

<p align="left side">
    <b>Chroma - the open-source embedding database</b>. <br />
    ä½¿ç”¨å†…å­˜æ„å»º Python æˆ– JavaScript LLM åº”ç”¨ç¨‹åºçš„æœ€å¿«æ–¹æ³•ï¼
</p>

  <a href="https://docs.trychroma.com/" target="_blank">
      Docs
  </a> |
  <a href="https://www.trychroma.com/" target="_blank">
      Homepage
  </a>
</p>


```
pip install chromadb # python client
# for javascript, npm install chromadb!
# for client-server mode, chroma run --path /chroma_db_path
```
### Python version

å®ƒéœ€è¦ **Python 3.8æˆ–ä¹‹åçš„ç‰ˆæœ¬**.

è¿™ä¸ªä¾‹å­è¿è¡Œåœ¨ Python 3.8.11åœ¨Windows 10 å’Œ Python 3.9åœ¨Macos 14.5ã€‚


## æ–‡ä»¶æŒ‡å¼•

ğŸ“Œ **Whole Flow** :å…¨æµç¨‹RAGç³»ç»Ÿä»£ç 

ğŸ“Œ **Two-Step Flow**:åˆ†ä¸¤æ­¥å®ç°RAGç³»ç»Ÿï¼Œå¯ä»¥æ›´æ–¹ä¾¿çš„å®ç°

ğŸ“Œ **liulangdiqiu.pdf**:æµæµªåœ°çƒ PDFæ ·æœ¬

ğŸ“Œ **readme**:è¯¦ç»†è®²è§£-ä¸­æ–‡å’Œè‹±æ–‡


## å¿«é€Ÿå¼€å§‹
### æ•´ä½“æµç¨‹å›¾
<div align="left side">
Â <img alt="Overall flow chart" height="px10" src="https://cdn-lfs-us-1.huggingface.co/repos/13/3d/133d8ca2460bf82ba2bdbe928d91a6c780364a6d0cf9005087db081cca492c02/ed22547b1538ea4fd18ea26777e14d9f7e51b3388b34d3cadf165cc37a7f63e0?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27RAG_workflow.png%3B+filename%3D%22RAG_workflow.png%22%3B&response-content-type=image%2Fpng&Expires=1720595234&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDU5NTIzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzEzLzNkLzEzM2Q4Y2EyNDYwYmY4MmJhMmJkYmU5MjhkOTFhNmM3ODAzNjRhNmQwY2Y5MDA1MDg3ZGIwODFjY2E0OTJjMDIvZWQyMjU0N2IxNTM4ZWE0ZmQxOGVhMjY3NzdlMTRkOWY3ZTUxYjMzODhiMzRkM2NhZGYxNjVjYzM3YTdmNjNlMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=LV%7E8WlQ2VBRcTeKDaFyH-ZiwsnQ76OKtGqa8vRzftKIY1qqY8vRNI7QP0H9ApdCXdBOVdMnmHMOJlq2yKnYPoQk7O5Q5O1dDcUPhbyuEvnuIIuoVNThmRxq-PCjbCuaaUuuU8jpMWW-K%7EFp2kssB2-KXX5NmLJ-qV7Xdl9jCdDCKr2JbyJN5Wach5p5rRlfh-sdB2a3vESqFI3pHvGgM7XCDbh1JmD0925Q7pvrRMXfFoBy-Ibvu31xiKnu%7EV%7EnvtVVtmtYTNrTSCI93xZ0DWhpSCOzoCP51wobl9fyoM5N9It4HXQeoxUoM8aA0wmirJLy7OtJC2yHuWnCNO3FLAg__&Key-Pair-Id=K24J24Z295AEI9">
</div>

<div align="right side">
Â <img alt="overall flow chart" height="px10" src="https://s2.loli.net/2024/07/07/OUPa9gmAxZ15elW.png">
</div>
è¿™çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªæå…¶å¤æ‚çš„è¿‡ç¨‹ï¼Œå¯¹å§ï¼Ÿ
æˆ‘ä»¬åˆ†åˆ«æ¥çœ‹ä¸€ä¸‹ã€‚

### 1.å‰æœŸæµç¨‹å›¾
<div align="center">
Â <img alt="flow chart" height="px150" src="https://python.langchain.com/v0.2/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png">
</div>

#### a.è£…è½½
Langchainæä¾›äº†è®¸å¤šå†…ç½®çš„æ–‡æ¡£åŠ è½½å™¨<a href="https://python.langchain.com/v0.2/docs/how_to/#document-loaders/" target="_blank">(Documents-loaders)</a>.

æ–‡æ¡£æ˜¯åŒ…å«æ–‡æœ¬å’ŒåŸå§‹æ•°æ®çš„å­—å…¸ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨PyMuPDFæ¥æ›´å¥½åœ°å¤„ç†PDFæ–‡ä»¶ä¸­çš„å†…å®¹ã€‚
```
import fitz  # PyMuPDF
from langchain.schema import Document

def read_pdf(file_path):
    doc = fitz.open(file_path)
    content = ""
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        content += page.get_text()
    return content

def pdf_to_documents(file_path):
    content = read_pdf(file_path)
    return [Document(page_content=content)]

file_path = "your storage location/liulang.pdf"
raw_documents = pdf_to_documents(file_path)
```
#### b.åˆ†å‰²/åˆ‡å—
æ–‡æ¡£åˆ†å—ï¼šæ–‡æœ¬åˆ†å‰²å™¨å°†æ–‡ä»¶æˆ–æ–‡æœ¬åˆ†å‰²æˆå—ï¼Œä»¥é˜²æ­¢æ–‡ä»¶ä¿¡æ¯è¶…è¿‡LLMä»¤ç‰Œã€‚è¿™ä¸€æ­¥å¸¸ç”¨çš„å·¥å…·æœ‰RecursiveCharacterTextSplitterå’ŒCharacterTextSplitterã€‚ä¸åŒä¹‹å¤„åœ¨äºï¼Œå¦‚æœå—å¤§å°è¶…è¿‡æŒ‡å®šé˜ˆå€¼ï¼ŒRecursiveCharacterTextSplitter ä¹Ÿä¼šé€’å½’åœ°å°†æ–‡æœ¬åˆ†å‰²æˆæ›´å°çš„å—ã€‚

ğŸ”¸chunk_size: ç¡®å®šåˆ†å‰²æ–‡æœ¬æ—¶æ¯ä¸ªå—ä¸­çš„æœ€å¤§å­—ç¬¦æ•°ã€‚å®ƒæŒ‡å®šæ¯ä¸ªå—çš„å¤§å°æˆ–é•¿åº¦ã€‚

ğŸ”¸chunk_overlap: ç¡®å®šåˆ†å‰²æ–‡æœ¬æ—¶è¿ç»­å—ä¹‹é—´é‡å çš„å­—ç¬¦æ•°ã€‚å®ƒæŒ‡å®šå‰ä¸€ä¸ªå—çš„å¤šå°‘å†…å®¹åº”åŒ…å«åœ¨ä¸‹ä¸€ä¸ªå—ä¸­ã€‚

ç¡®å®šåˆ†å‰²æ–‡æœ¬æ—¶è¿ç»­å—ä¹‹é—´é‡å çš„å­—ç¬¦æ•°ã€‚å®ƒæŒ‡å®šå‰ä¸€ä¸ªå—çš„å¤šå°‘å†…å®¹åº”åŒ…å«åœ¨ä¸‹ä¸€ä¸ªå—ä¸­ã€‚

Langchainè¿˜æä¾›äº†å¤šç§æ–‡æœ¬åˆ†å‰²å™¨ä¾›ä½ é€‰æ‹©ã€‚<a href="https://python.langchain.com/v0.2/docs/how_to/#text-splitters">(Text Splitters)</a>.

ğŸŸ¢BTW:è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½ç”¨çš„å°å·¥å…·ï¼Œå¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°æ–‡æœ¬åˆ†å—æƒ…å†µã€‚<a href="https://chunkviz.up.railway.app/">(Chunkviz)</a>.
```
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
all_splits = text_splitter.split_documents(raw_documents)
```
#### c.åµŒå…¥
Embeddingï¼šç„¶åä½¿ç”¨Embeddingå°†ä¸Šä¸€æ­¥åˆ’åˆ†çš„å—æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ã€‚LangChainæä¾›äº†å¾ˆå¤šEmbeddingæ¨¡å‹æ¥å£ï¼Œæ¯”å¦‚OpenAIã€Cohereã€Hugging Faceã€Weaviateç­‰ï¼Œå¯ä»¥å‚è€ƒLangChainå®˜ç½‘ã€‚<a href="https://python.langchain.com/v0.2/docs/how_to/#embedding-models">(Embedding)</a>.

è¿™é‡Œä½¿ç”¨ç®€å•æ–¹ä¾¿çš„ OllamaåµŒå…¥æ¨¡å‹ã€‚
```bash
from langchain_community.embeddings import OllamaEmbeddings

embedding_model = OllamaEmbeddings()
```
#### d.å‘é‡æ•°æ®åº“

Storeï¼šæˆ‘ä»¬å°†Embeddingåçš„ç»“æœå­˜å‚¨åœ¨VectorDBä¸­ã€‚å¸¸è§çš„VectorDBæœ‰Chromaã€Pineconeã€FAISSç­‰ï¼Œè¿™é‡Œæˆ‘ä½¿ç”¨Chromaæ¥å®ç°ã€‚ Chromaä¸LangChainé›†æˆè‰¯å¥½ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨LangChainçš„ç•Œé¢ã€‚<a href="https://python.langchain.com/v0.2/docs/how_to/#vector-stores">(Vector-stores)</a>.

ä»åŒ…å«å¤šä¸ª Document å¯¹è±¡çš„åˆ—è¡¨ all_splits ä¸­æå–æ¯ä¸ªæ–‡æ¡£çš„æ–‡æœ¬å†…å®¹ï¼Œå¹¶å°†å†…å®¹å­˜å‚¨åœ¨åˆ—è¡¨ text ä¸­ã€‚
```
texts = [doc.page_content for doc in all_splits]
```
çŸ¢é‡æ•°æ®åº“æŒä¹…åŒ–å­˜å‚¨è·¯å¾„
```
persist_directory = 'your storage location/chroma_db'
```
è¿™æ®µä»£ç ä½¿ç”¨Chromaç±»å°†æ–‡æœ¬åˆ—è¡¨æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œå°†è¿™äº›å‘é‡å­˜å‚¨åœ¨åä¸ºâ€œRAG_chromaâ€çš„é›†åˆä¸­ï¼Œå­˜å‚¨è·¯å¾„ä¸º'your storage location/chroma_db'
```
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma

vectorstore = Chroma.from_texts(
    texts=texts,
    embedding=embedding_model,
    collection_name="RAG_chroma",
    persist_directory=persist_directory
)
```
### 2.Flow chart(In-production)

<div align="center">
Â <img alt="overall flow chart" height="px10" src="https://python.langchain.com/v0.2/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png">
</div>

#### z.Load(Optional)

If you want to call the database at any time to implement Q&A, you must load the contents of the database first.

```
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma

chroma_db_path = "your storage location\chroma_db"
embedding_model = OllamaEmbeddings()
vectorstore = Chroma.from_texts(
    embedding=embedding_model,
    texts=chroma_db_path,
    collection_name="RAG_chroma"
)
```
#### ğŸ“ŒTips- Sorting process

Above we imported the PDF information into the DB and started the LLM service. Next we need to string together the entire RAG steps:

1ï¸âƒ£ Users sent QA

2ï¸âƒ£ Text Retrieval from Chroma_DB

3ï¸âƒ£ Combine QA with Text Retrieval and send to LLM

4ï¸âƒ£ LLM answers based on information

#### a.Retriever

First we need to create a Retriever, which can return corresponding files based on unstructured QA. LangChain provides many methods and integrates third-party tools. I use the Vectorstore method here. For other types, you can refer to <a href="https://python.langchain.com/v0.2/docs/how_to/#retrievers/">(Retriever)</a>.

```
retriever = vectorstore.as_retriever()
```

#### b.Prompt templates

Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages. The reason this PromptValue exists is to make it easy to switch between strings and messages.

There are a few different types of prompt templates.<a href="https://python.langchain.com/v0.2/docs/concepts/#prompt-templates">(prompt-templates)</a>.
This example uses ChatPromptTemplate.

##### b1.Template
Template defines a structure that formats the input data (context and question) and generates a complete text template, which is then passed to a large language model (LLM) to generate the final answer.

â€¢ {context}: This placeholder will be replaced by the relevant context retrieved from the vector store.

â€¢ {question}: This placeholder will be replaced by the entered question from users.
```
template = """Answer the question with Chinese and based only on the following context:
{context}

Question: {question}
"""
```
ğŸ“According to the PDF content in our example

â€¢context:â€œèˆªè¡Œå§”å‘˜ä¼šçš„æœ€æ–°è®¡åˆ’æ˜¯å¢åŠ èˆ¹åªæ•°é‡å¹¶æ‰©å±•èˆªçº¿ã€‚â€

â€¢question:â€œèˆªè¡Œå§”å‘˜ä¼šçš„è®¡åˆ’æ˜¯ä»€ä¹ˆï¼Ÿâ€

The template generates text formatted as follows:
```
Answer the question with Chinese and based only on the following context:
èˆªè¡Œå§”å‘˜ä¼šçš„æœ€æ–°è®¡åˆ’æ˜¯å¢åŠ èˆ¹åªæ•°é‡å¹¶æ‰©å±•èˆªçº¿ã€‚

Question: èˆªè¡Œå§”å‘˜ä¼šçš„è®¡åˆ’æ˜¯ä»€ä¹ˆï¼Ÿ
```
##### b2.Prompt

Prompt plays the role of constructing and formatting input data in the code, so that the input data can be passed to the large language model (LLM) in a predetermined template format for processing. Specifically, prompt uses a defined template, inserts the actual context and question into the placeholder position in the template, and generates a complete input text for use by the large language model.

Creates a ChatPromptTemplate instance prompt based on the previously defined template template.
```
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template(template)
```

#### c.LLM

Instantiate a large language model llm with the model name "qwen:4b".

```
from langchain_community.llms import Ollama

llm = Ollama(model="qwen:4b")
```

#### d.Chain

A processing chain chain is constructed, and its workflow is as follows:

1. Use RunnableParallel to handle two tasks in parallel: the problem of getting context from the retriever and passing the input.
2. Pass the context and question to the template prompt to generate a complete answer template.
3. Use the large language model llm to generate answers based on the template.
4. Use StrOutputParser to parse the answer into a string.

```
from langchain_community.llms import Ollama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

chain = (
        RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
        | prompt
        | llm
        | StrOutputParser()
)
```

#### e.users query and LLM answer

A data model Question is defined, which contains a string root attribute __root__ to represent the input question.

Configure the processing chain to accept input of type Question.

Call the processing chain chain with the input question "èˆªè¡Œå§”å‘˜ä¼šçš„è®¡åˆ’æ˜¯ä»€ä¹ˆï¼Ÿ" and print out the answer.
```
from langchain_core.pydantic_v1 import BaseModel

class Question(BaseModel):
    __root__: str
chain = chain.with_types(input_type=Question)
output = chain.invoke("èˆªè¡Œå§”å‘˜ä¼šçš„è®¡åˆ’æ˜¯ä»€ä¹ˆï¼Ÿ")
print(output)
```
### 3.Results

ğŸ”´In the **whole flow** code run, the running time  is **283** seconds by using **Nvida 1660ti GPU cuda**.

Result:èˆªè¡Œå§”å‘˜ä¼šçš„è®¡åˆ’æ˜¯ä¸ºäººç±»åœ¨æœªæ¥å»ºç«‹æ–°å®¶å›­çš„è¿‡ç¨‹ä¸­æä¾›ç§‘å­¦åˆç†çš„è·¯çº¿ã€æ—¶é—´å’Œç©ºé—´çš„è§„åˆ’ã€‚

ğŸ”´In the **whole flow** code run, the running time  is **26** seconds by using **Apple Silicon M2max GPU**.

Result:èˆªè¡Œå§”å‘˜ä¼šçš„è®¡åˆ’æ˜¯åœ¨åœ°çƒç»•å¤ªé˜³å…¬è½¬çš„è¿‡ç¨‹ä¸­è¿›è¡Œç§‘å­¦ç ”ç©¶å’ŒæŠ€æœ¯å¼€å‘ï¼Œä»¥æœŸåœ¨ä¸ä¹…çš„å°†æ¥èƒ½å¤Ÿå®ç°äººç±»çš„é•¿æœŸå’Œå¹³å’Œå‘å±•ã€‚

You will definitely be curious why the test results are not consistent with the answers in the original article. No doubt, this is indeed the case. This is because the final result delivered by LLM will be affected by the text segmenter and text embedding in the process, which requires special optimization and adjustment of the details of each component to obtain accurate answers.
